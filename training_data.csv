id,model_name,quant_level,pass_type,pass_count,iteration,timestamp,model_size_mb,response_fluency,emotional_intensity,emotional_match,empathy_score,metaphor_usage,sentiment_accuracy,overall_score,notes,config_hash
3,llama2_13b,gptq,pass_1,1,1,2025-07-29T17:43:16.769665,13200.0,0.87,0.8,0.84,0.81,0.73,0.81,0.8200000000000001,GPTQ quantization balanced approach,
2,llama2_13b,8bit,pass_1,1,1,2025-07-29T17:43:06.842983,15000.0,0.88,0.82,0.85,0.83,0.75,0.82,0.8344999999999999,8-bit quantization with better emotional preservation,
1,llama2_13b,4bit,pass_1,1,1,2025-07-29T17:42:36.029053,12500.0,0.85,0.78,0.82,0.79,0.72,0.8,0.802,Initial 4-bit quantization attempt,
