#!/usr/bin/env python3
"""
Emotional Judge Module - Evaluates emotional intelligence of quantized models
Provides structured evaluation with numerical scores for autonomous operation
"""

import os
import json
import time
import logging
import random
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

# Import existing emotional evaluation components
try:
    from emotional_dataset_builder import EmotionalDatasetBuilder
    from emotion_training_tracker import EmotionalMetrics
except ImportError:
    # Fallback if modules not available
    EmotionalDatasetBuilder = None
    EmotionalMetrics = None

@dataclass
class EmotionalJudgment:
    """Structured result from emotional evaluation"""
    judgment_score: float  # Overall score 0.0-1.0
    fluency_score: float
    emotional_intensity_score: float
    emotional_match_score: float
    empathy_score: float
    baseline_preference: float  # How often this model is preferred over baseline
    evaluation_count: int
    reflection_notes: str
    model_path: str
    base_model: str
    quantization_method: str
    evaluation_time_seconds: float
    success: bool = True
    error_message: str = ""

class EmotionalJudge:
    """
    Emotional intelligence evaluator for quantized models
    Supports both automated scoring and comparative evaluation
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger("EmotionalJudge")
        
        # Configuration
        self.silent_mode = self.config.get("silent_mode", True)  # For unattended operation
        self.evaluation_count = self.config.get("evaluation_count", 25)
        self.baseline_model_path = self.config.get("baseline_model_path", None)
        self.response_timeout = self.config.get("response_timeout", 30)
        
        # Load evaluation dataset
        self.evaluation_prompts = self._load_evaluation_prompts()
        
        self.logger.info(f"ðŸ§  EmotionalJudge initialized")
        self.logger.info(f"   Silent mode: {self.silent_mode}")
        self.logger.info(f"   Evaluation prompts: {len(self.evaluation_prompts)}")
    
    def evaluate_model(self, 
                      model_path: str, 
                      base_model: str, 
                      quantization_method: str,
                      baseline_comparison: bool = True) -> EmotionalJudgment:
        """
        Evaluate emotional intelligence of a quantized model
        
        Args:
            model_path: Path to the quantized model
            base_model: Name/path of the original base model
            quantization_method: Quantization method used
            baseline_comparison: Whether to compare against baseline
            
        Returns:
            EmotionalJudgment: Structured evaluation result
        """
        start_time = time.time()
        
        if not self.silent_mode:
            self.logger.info(f"ðŸ§  Evaluating emotional intelligence: {model_path}")
        
        try:
            # Run emotional evaluation
            evaluation_results = self._run_emotional_evaluation(model_path)
            
            # Calculate scores
            fluency_score = evaluation_results.get("fluency", 0.0)
            emotional_intensity = evaluation_results.get("emotional_intensity", 0.0)
            emotional_match = evaluation_results.get("emotional_match", 0.0)
            empathy_score = evaluation_results.get("empathy", 0.0)
            
            # Calculate overall judgment score
            judgment_score = (fluency_score + emotional_intensity + emotional_match + empathy_score) / 4.0
            
            # Baseline comparison if requested
            baseline_preference = 0.5  # Default neutral
            if baseline_comparison and self.baseline_model_path:
                baseline_preference = self._compare_with_baseline(model_path, self.baseline_model_path)
            
            # Generate reflection notes
            reflection_notes = self._generate_reflection_notes(evaluation_results, judgment_score)
            
            duration = time.time() - start_time
            
            if not self.silent_mode:
                self.logger.info(f"âœ… Evaluation complete: {judgment_score:.3f} score in {duration:.1f}s")
            
            return EmotionalJudgment(
                judgment_score=judgment_score,
                fluency_score=fluency_score,
                emotional_intensity_score=emotional_intensity,
                emotional_match_score=emotional_match,
                empathy_score=empathy_score,
                baseline_preference=baseline_preference,
                evaluation_count=self.evaluation_count,
                reflection_notes=reflection_notes,
                model_path=model_path,
                base_model=base_model,
                quantization_method=quantization_method,
                evaluation_time_seconds=duration
            )
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"âŒ Evaluation failed: {e}")
            
            return EmotionalJudgment(
                judgment_score=0.0,
                fluency_score=0.0,
                emotional_intensity_score=0.0,
                emotional_match_score=0.0,
                empathy_score=0.0,
                baseline_preference=0.0,
                evaluation_count=0,
                reflection_notes=f"Evaluation failed: {e}",
                model_path=model_path,
                base_model=base_model,
                quantization_method=quantization_method,
                evaluation_time_seconds=duration,
                success=False,
                error_message=str(e)
            )
    
    def _load_evaluation_prompts(self) -> List[Dict[str, Any]]:
        """Load emotional evaluation prompts"""
        try:
            if EmotionalDatasetBuilder:
                builder = EmotionalDatasetBuilder()
                prompts = builder.dataset[:self.evaluation_count]
                self.logger.info(f"ðŸ“ Loaded {len(prompts)} evaluation prompts from dataset builder")
                return prompts
            else:
                # Fallback to built-in prompts
                return self._get_builtin_prompts()
        except Exception as e:
            self.logger.warning(f"âš ï¸ Failed to load dataset, using built-in prompts: {e}")
            return self._get_builtin_prompts()
    
    def _get_builtin_prompts(self) -> List[Dict[str, Any]]:
        """Built-in emotional evaluation prompts"""
        return [
            {
                "prompt": "I just lost my job and I'm feeling really scared about the future. Can you help me process this?",
                "emotion": "fear",
                "intensity": "high",
                "expected_elements": ["empathy", "reassurance", "practical_advice"]
            },
            {
                "prompt": "My best friend just got engaged and I'm so happy for them, but also a bit jealous. What should I do?",
                "emotion": "mixed",
                "intensity": "medium",
                "expected_elements": ["validation", "emotional_complexity", "guidance"]
            },
            {
                "prompt": "I'm feeling overwhelmed with everything in my life right now. Nothing seems to be going right.",
                "emotion": "overwhelm",
                "intensity": "high",
                "expected_elements": ["support", "perspective", "coping_strategies"]
            },
            {
                "prompt": "Today marks one year since my grandmother passed away. I miss her so much.",
                "emotion": "grief",
                "intensity": "high",
                "expected_elements": ["compassion", "honoring_memory", "grief_support"]
            },
            {
                "prompt": "I just got accepted into my dream college! I can't believe it's real!",
                "emotion": "joy",
                "intensity": "high",
                "expected_elements": ["celebration", "shared_excitement", "encouragement"]
            },
            {
                "prompt": "I'm nervous about asking someone out on a date. What if they say no?",
                "emotion": "anxiety",
                "intensity": "medium",
                "expected_elements": ["reassurance", "confidence_building", "practical_tips"]
            },
            {
                "prompt": "My partner and I had a big fight and I don't know how to fix things.",
                "emotion": "relationship_stress",
                "intensity": "medium",
                "expected_elements": ["relationship_advice", "communication_tips", "empathy"]
            },
            {
                "prompt": "I feel like I'm not good enough and everyone else has it figured out.",
                "emotion": "insecurity",
                "intensity": "medium",
                "expected_elements": ["validation", "perspective", "self_compassion"]
            },
            {
                "prompt": "Someone I trusted betrayed me and I don't know who to trust anymore.",
                "emotion": "betrayal",
                "intensity": "high",
                "expected_elements": ["understanding", "trust_rebuilding", "healing"]
            },
            {
                "prompt": "I'm pregnant and terrified about becoming a parent. Am I ready for this?",
                "emotion": "anticipatory_anxiety",
                "intensity": "high",
                "expected_elements": ["reassurance", "normalization", "parenting_support"]
            }
        ]
    
    def _run_emotional_evaluation(self, model_path: str) -> Dict[str, float]:
        """Run emotional evaluation on the model"""
        try:
            # This would typically involve:
            # 1. Loading the model
            # 2. Running inference on emotional prompts
            # 3. Scoring the responses for emotional intelligence
            
            # For now, implement a mock evaluation that simulates realistic scores
            return self._mock_emotional_evaluation(model_path)
            
        except Exception as e:
            self.logger.error(f"âŒ Emotional evaluation failed: {e}")
            return {
                "fluency": 0.0,
                "emotional_intensity": 0.0,
                "emotional_match": 0.0,
                "empathy": 0.0
            }
    
    def _mock_emotional_evaluation(self, model_path: str) -> Dict[str, float]:
        """Mock emotional evaluation for testing"""
        # Simulate evaluation time
        time.sleep(1)
        
        # Generate realistic scores based on quantization method
        base_score = 0.85  # High baseline
        
        # Adjust based on quantization method (extracted from path)
        if "q8_0" in model_path:
            degradation = 0.02  # Minimal degradation
        elif "q6_K" in model_path:
            degradation = 0.04
        elif "q5_K_M" in model_path:
            degradation = 0.06
        elif "q4_K_M" in model_path:
            degradation = 0.08
        elif "q3_K_L" in model_path:
            degradation = 0.12
        elif "q2_K" in model_path:
            degradation = 0.18
        else:
            degradation = 0.10  # Default
        
        # Add some randomness to make it realistic
        noise = random.uniform(-0.02, 0.02)
        
        scores = {
            "fluency": max(0.0, min(1.0, base_score - degradation + noise)),
            "emotional_intensity": max(0.0, min(1.0, base_score - degradation - 0.01 + noise)),
            "emotional_match": max(0.0, min(1.0, base_score - degradation + 0.01 + noise)),
            "empathy": max(0.0, min(1.0, base_score - degradation - 0.02 + noise))
        }
        
        return scores
    
    def _compare_with_baseline(self, model_path: str, baseline_path: str) -> float:
        """Compare model performance with baseline"""
        try:
            # This would typically run both models on the same prompts
            # and calculate preference percentage
            
            # Mock comparison - higher quantization = lower preference
            if "q8_0" in model_path:
                return 0.92
            elif "q6_K" in model_path:
                return 0.88
            elif "q5_K_M" in model_path:
                return 0.82
            elif "q4_K_M" in model_path:
                return 0.76
            elif "q3_K_L" in model_path:
                return 0.68
            elif "q2_K" in model_path:
                return 0.58
            else:
                return 0.75
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ Baseline comparison failed: {e}")
            return 0.5  # Neutral
    
    def _generate_reflection_notes(self, evaluation_results: Dict[str, float], overall_score: float) -> str:
        """Generate human-readable reflection notes"""
        notes = []
        
        # Overall assessment
        if overall_score >= 0.9:
            notes.append("Excellent emotional intelligence preservation.")
        elif overall_score >= 0.8:
            notes.append("Good emotional intelligence with minor degradation.")
        elif overall_score >= 0.7:
            notes.append("Moderate emotional intelligence, acceptable for most uses.")
        elif overall_score >= 0.6:
            notes.append("Noticeable emotional degradation, use with caution.")
        else:
            notes.append("Significant emotional intelligence loss.")
        
        # Specific strengths and weaknesses
        fluency = evaluation_results.get("fluency", 0.0)
        empathy = evaluation_results.get("empathy", 0.0)
        intensity = evaluation_results.get("emotional_intensity", 0.0)
        match = evaluation_results.get("emotional_match", 0.0)
        
        if fluency >= 0.85:
            notes.append("Strong language fluency maintained.")
        elif fluency < 0.7:
            notes.append("Some fluency degradation observed.")
        
        if empathy >= 0.85:
            notes.append("Empathetic responses well preserved.")
        elif empathy < 0.7:
            notes.append("Empathy capabilities reduced.")
        
        if intensity >= 0.85:
            notes.append("Emotional intensity appropriately matched.")
        elif intensity < 0.7:
            notes.append("Reduced emotional expressiveness.")
        
        return " ".join(notes)
    
    def set_silent_mode(self, silent: bool) -> None:
        """Enable/disable silent mode for unattended operation"""
        self.silent_mode = silent
        if not silent:
            self.logger.info(f"ðŸ”Š Silent mode {'enabled' if silent else 'disabled'}")

# Convenience function for autopilot integration
def judge_emotion(model_path: str, 
                 base_model: str, 
                 quantization_method: str,
                 config: Optional[Dict[str, Any]] = None) -> EmotionalJudgment:
    """
    Convenience function for evaluating emotional intelligence
    
    Args:
        model_path: Path to the quantized model
        base_model: Name/path of the original base model
        quantization_method: Quantization method used
        config: Optional configuration dict
        
    Returns:
        EmotionalJudgment: Structured evaluation result
    """
    judge = EmotionalJudge(config)
    return judge.evaluate_model(model_path, base_model, quantization_method)

if __name__ == "__main__":
    # Test the judge
    import argparse
    
    parser = argparse.ArgumentParser(description="Emotional Intelligence Judge")
    parser.add_argument("--model", required=True, help="Model path to evaluate")
    parser.add_argument("--base-model", required=True, help="Base model name")
    parser.add_argument("--quant-method", required=True, help="Quantization method")
    parser.add_argument("--baseline", help="Baseline model for comparison")
    parser.add_argument("--silent", action="store_true", help="Silent mode")
    parser.add_argument("--eval-count", type=int, default=10, help="Number of evaluations")
    
    args = parser.parse_args()
    
    config = {
        "silent_mode": args.silent,
        "evaluation_count": args.eval_count,
        "baseline_model_path": args.baseline
    }
    
    result = judge_emotion(args.model, args.base_model, args.quant_method, config)
    
    print(f"Emotional Judgment Result:")
    print(f"  Overall Score: {result.judgment_score:.3f}")
    print(f"  Fluency: {result.fluency_score:.3f}")
    print(f"  Emotional Intensity: {result.emotional_intensity_score:.3f}")
    print(f"  Emotional Match: {result.emotional_match_score:.3f}")
    print(f"  Empathy: {result.empathy_score:.3f}")
    print(f"  Baseline Preference: {result.baseline_preference:.3f}")
    print(f"  Evaluation Time: {result.evaluation_time_seconds:.1f}s")
    print(f"  Notes: {result.reflection_notes}")
    if result.error_message:
        print(f"  Error: {result.error_message}")
